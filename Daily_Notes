-------------------------------------------------------------------------------------------
ok. you figured a big part on out how MultiHeadedAttn works.
In particular, you learned how nn.Linear() might accept N-D tensors and 
perform fc only on last dimension. 
You followed the computational starting from 
EncoderDecoder.encode -> Encoder.forward -> EncoderLayer.forward -> ResidualConnection.forward
-> MultiHeadedAttention.forward -> tensor computations and transformations.

Next step is to finish MultiHeadedAttention.forward function and continue the
computational stack chain.





-------------------------------------------------------------------------------------------

ok. you've done quite a bit part today.
first of all, you managed to initialize the model
second, at most important you managed to start inference (until MultiHeadedAttention).

also, you organised your python-dev workspace: pyscaffold, github, ipynb in VSCode.
For degubbingm you no longer need to push code and re-install in colab session!!

Next step is to debug MultiHeadedAttention.

as always, the very first module to start is 'encode' function of EncoderDecoder class.

first of all it applies src_emb (Embeddings + PosEncoding) that you already figured out.

then (Encoder.py) it creates a loop of N iters of x=layer(x, mask)

	this is brake down to (EncoderLayer.py)

	layer = EncoderLayer(size=d_model, self_attn = attn, src_attn = attn, feed_fwd = ff)

	so you next step is to go to EncoderLayer def and flow through its forward function,

	where your input x is basically tensor (1 x 10 x 16) and mask (1 x 10)

	use you A2 paper and pen. you might want to transfer A2 to draw.io later

