{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21815,
     "status": "ok",
     "timestamp": 1667234557531,
     "user": {
      "displayName": "Kapchenko Nikita",
      "userId": "13103833174454489193"
     },
     "user_tz": -60
    },
    "id": "SaOlNWG3sIxo",
    "outputId": "bbc85c13-4edc-4950-8b40-97b0ac0a49c8"
   },
   "outputs": [],
   "source": [
    "# Use below line for demo in external colabs\n",
    "# !pip install -q torchdata==0.3.0 torchtext==0.12 spacy==3.2 altair GPUtil\n",
    "# !python -m spacy download de_core_news_sm\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !pip install -q git+https://github.com/nikitakapitan/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 381,
     "status": "ok",
     "timestamp": 1667235645442,
     "user": {
      "displayName": "Kapchenko Nikita",
      "userId": "13103833174454489193"
     },
     "user_tz": -60
    },
    "id": "WN8Jccx1sYXG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from transformers.main import make_model\n",
    "\n",
    "from transformers.data.token import load_tokenizers\n",
    "from transformers.data.vocab import load_vocab\n",
    "from transformers.data.dataloader import create_dataloaders\n",
    "\n",
    "from transformers.Embeddings import Embeddings\n",
    "from transformers.PositionalEncoding import PositionalEncoding\n",
    "from transformers.MultiHeadedAttention import MultiHeadedAttention\n",
    "from transformers.helper import following_mask\n",
    "from transformers.LayerNorm import LayerNorm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 1041,
     "status": "ok",
     "timestamp": 1667234558567,
     "user": {
      "displayName": "Kapchenko Nikita",
      "userId": "13103833174454489193"
     },
     "user_tz": -60
    },
    "id": "_hV9cbyH_59h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished.\n",
      "Vocabulary sizes:\n",
      "len: SRC=8315 TGT=6384\n"
     ]
    }
   ],
   "source": [
    "spacy_de, spacy_en = load_tokenizers()\n",
    "vocab_src, vocab_tgt = load_vocab(spacy_de=spacy_de, spacy_en=spacy_en)\n",
    "\n",
    "_, valid_dataloader = create_dataloaders(\n",
    "        torch.device(\"cpu\"),\n",
    "        vocab_src,\n",
    "        vocab_tgt,\n",
    "        spacy_de,\n",
    "        spacy_en,\n",
    "        batch_size=1,\n",
    "        is_distributed=False,\n",
    "    )\n",
    "\n",
    "architecture = {\n",
    "        'src_vocab_len' : len(vocab_src),\n",
    "        'tgt_vocab_len' : len(vocab_tgt),\n",
    "        'N' : 6, # loop\n",
    "        'd_model' : 512, # emb\n",
    "        'd_ff' : 2048,\n",
    "        'h' : 8,\n",
    "        'p_dropout' : 0.1\n",
    "    }\n",
    "\n",
    "model = make_model(\n",
    "        src_vocab_len = architecture['src_vocab_len'], \n",
    "        tgt_vocab_len = architecture['tgt_vocab_len'], \n",
    "        N=6,\n",
    "        d_model=architecture['d_model'], \n",
    "        d_ff = architecture['d_ff'],\n",
    "        h = architecture['h'],\n",
    "        dropout=architecture['p_dropout'])\n",
    "\n",
    "\n",
    "model.eval() # switch to eval (no DropOut & BatchNorm learning)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocab()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "embedding(): argument 'indices' (position 2) must be Tensor, not Vocab",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m memory \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mencode(vocab_src, \u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/GitHub/transformers/src/transformers/EncoderDecoder.py:33\u001b[0m, in \u001b[0;36mEncoderDecoder.encode\u001b[0;34m(self, src, src_mask)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mencode\u001b[39m(\u001b[39mself\u001b[39m, src, src_mask):\n\u001b[0;32m---> 33\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mencoder(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msrc_emb(src), src_mask)\n",
      "File \u001b[0;32m~/GitHub/venvai/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/GitHub/venvai/lib/python3.10/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/GitHub/venvai/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/GitHub/transformers/src/transformers/Embeddings.py:13\u001b[0m, in \u001b[0;36mEmbeddings.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 13\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49memb(x) \u001b[39m*\u001b[39m math\u001b[39m.\u001b[39msqrt(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39md_model)\n",
      "File \u001b[0;32m~/GitHub/venvai/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/GitHub/venvai/lib/python3.10/site-packages/torch/nn/modules/sparse.py:158\u001b[0m, in \u001b[0;36mEmbedding.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 158\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49membedding(\n\u001b[1;32m    159\u001b[0m         \u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpadding_idx, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_norm,\n\u001b[1;32m    160\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnorm_type, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscale_grad_by_freq, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msparse)\n",
      "File \u001b[0;32m~/GitHub/venvai/lib/python3.10/site-packages/torch/nn/functional.py:2183\u001b[0m, in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2177\u001b[0m     \u001b[39m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[1;32m   2178\u001b[0m     \u001b[39m# XXX: equivalent to\u001b[39;00m\n\u001b[1;32m   2179\u001b[0m     \u001b[39m# with torch.no_grad():\u001b[39;00m\n\u001b[1;32m   2180\u001b[0m     \u001b[39m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[1;32m   2181\u001b[0m     \u001b[39m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[1;32m   2182\u001b[0m     _no_grad_embedding_renorm_(weight, \u001b[39minput\u001b[39m, max_norm, norm_type)\n\u001b[0;32m-> 2183\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49membedding(weight, \u001b[39minput\u001b[39;49m, padding_idx, scale_grad_by_freq, sparse)\n",
      "\u001b[0;31mTypeError\u001b[0m: embedding(): argument 'indices' (position 2) must be Tensor, not Vocab"
     ]
    }
   ],
   "source": [
    "memory = model.encode(vocab_src, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CezVbVlt_bjl"
   },
   "source": [
    "# Down-brake **Model.encode(src, src_mask)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 381,
     "status": "ok",
     "timestamp": 1667237041900,
     "user": {
      "displayName": "Kapchenko Nikita",
      "userId": "13103833174454489193"
     },
     "user_tz": -60
    },
    "id": "oA9cbRs5_YyA",
    "outputId": "96f2ae2c-a05a-4588-916e-1f24b5487f83"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Vocab' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m src \u001b[39m=\u001b[39m vocab_src\n\u001b[0;32m----> 2\u001b[0m src\u001b[39m.\u001b[39;49mshape\n",
      "File \u001b[0;32m~/GitHub/venvai/lib/python3.10/site-packages/torch/nn/modules/module.py:1185\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1183\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1184\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1185\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1186\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Vocab' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "src = vocab_src\n",
    "src.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1667237042924,
     "user": {
      "displayName": "Kapchenko Nikita",
      "userId": "13103833174454489193"
     },
     "user_tz": -60
    },
    "id": "zsy6CDaDnVZI",
    "outputId": "f8869c79-8c3c-4773-9095-abe6b5237a78"
   },
   "outputs": [],
   "source": [
    "emb = Embeddings(vocab=FRENCH, d_model=D_MODEL)\n",
    "x = emb(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rl8KmYjjAYpu"
   },
   "source": [
    "## Step 2 PositionEncoding(d_model, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "executionInfo": {
     "elapsed": 308,
     "status": "error",
     "timestamp": 1667237050666,
     "user": {
      "displayName": "Kapchenko Nikita",
      "userId": "13103833174454489193"
     },
     "user_tz": -60
    },
    "id": "Znjm0RX37t3h",
    "outputId": "cd699ef8-2d7f-41d9-b06d-2d6be65e777c"
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 5000\n",
    "pos_enc = PositionalEncoding(d_model=D_MODEL, dropout=DROPOUT, max_len=MAX_LEN)\n",
    "x = pos_enc(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Start step 3 ResidualConnection (input 3)\n",
    "## Step 4 MultiHeadedAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 291,
     "status": "ok",
     "timestamp": 1667238034167,
     "user": {
      "displayName": "Kapchenko Nikita",
      "userId": "13103833174454489193"
     },
     "user_tz": -60
    },
    "id": "xevj1lyzCiy8",
    "outputId": "82e18b16-fa55-4513-9e6d-bd388c1076a7"
   },
   "outputs": [],
   "source": [
    "attn = MultiHeadedAttention(h=H, d_model=D_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.1 Query, Key, Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "maylguAWDMwY"
   },
   "outputs": [],
   "source": [
    "# MultiHeadedAttention.__init__\n",
    "d_head = D_MODEL // H\n",
    "h = H\n",
    "q_fc = nn.Linear(D_MODEL, D_MODEL)\n",
    "k_fc = nn.Linear(D_MODEL, D_MODEL)\n",
    "v_fc = nn.Linear(D_MODEL, D_MODEL)\n",
    "final_fc = nn.Linear(D_MODEL, D_MODEL)\n",
    "dropout = nn.Dropout(p=DROPOUT)\n",
    "\n",
    "input_3 = x\n",
    "mask = src_mask\n",
    "\n",
    "# MultiHeadedAttention.forward : compute Query, Key, Value\n",
    "mask = mask.unsqueeze(1)\n",
    "print('mask shape=',mask.shape)\n",
    "n_batches = input_3.size(0) # 1\n",
    "\n",
    "print('input.shape=', [mapa[e] for e in input_3.shape])\n",
    "query = q_fc(input_3)\n",
    "key = k_fc(input_3)\n",
    "value = v_fc(input_3)\n",
    "\n",
    "print('query.shape=', [mapa[e] for e in query.shape])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.2 Split to H heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into H heads.\n",
    "query = query.view(n_batches, n, h, d_head) .transpose(1, 2)\n",
    "key = key.view(n_batches, n, h, d_head).transpose(1, 2)\n",
    "value = value.view(n_batches, n, h, d_head).transpose(1, 2)\n",
    "query.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.3 Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def attention\n",
    "key_t = key.transpose(-2, -1)\n",
    "print('query.shape=', [mapa[e] for e in query.shape])\n",
    "print('key_t.shape=', [mapa[e] for e in key_t.shape])\n",
    "\n",
    "scores = torch.matmul(query, key_t) / math.sqrt(d_head)\n",
    "print('scores.shape=', [mapa[e] for e in scores.shape])\n",
    "\n",
    "scores = scores.masked_fill(mask, -1e9)\n",
    "\n",
    "p_attn = scores.softmax(dim=-1)\n",
    "print('p_attn.shape=', [mapa[e] for e in p_attn.shape])\n",
    "\n",
    "# if dropout is not None:\n",
    "#     p_attn = dropout(p_attn)\n",
    "\n",
    "print('value.shape=', [mapa[e] for e in value.shape])\n",
    "headed_context = torch.matmul(p_attn, value)\n",
    "print('headed_context.shape=', [mapa[e] for e in headed_context.shape])\n",
    "\n",
    "context = headed_context.transpose(1,2).contiguous().view(n_batches, n, h * d_head)\n",
    "print('context.shape=', [mapa[e] for e in context.shape])\n",
    "\n",
    "output_4 = final_fc(context)\n",
    "print('output_4.shape=', [mapa[e] for e in output_4.shape])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = LayerNorm(D_MODEL)\n",
    "output_5 = norm(output_4)\n",
    "print('output_5.shape=', [mapa[e] for e in output_5.shape])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 Residual Connectnion (output 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# end ResidualConnection\n",
    "output_3 = input_3 + output_5\n",
    "print('output_3.shape=', [mapa[e] for e in output_3.shape])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Start Step 6 Residual Connecction (input 6)\n",
    "\n",
    "## Step 7 PositionalWiseFeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_1 = nn.Linear(D_MODEL, D_FF)\n",
    "w_2 = nn.Linear(D_FF, D_MODEL)\n",
    "\n",
    "fc1 = w_1(output_3).relu() # + DropOut\n",
    "print('fc1.shape=', [mapa[e] for e in fc1.shape])\n",
    "output_7 = w_2(fc1)\n",
    "print('output_7.shape=', [mapa[e] for e in output_7.shape])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm = LayerNorm(D_MODEL)\n",
    "output_8 = norm(output_7)\n",
    "print('output_8.shape=', [mapa[e] for e in output_8.shape])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 ResidualConnection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_6 = output_8 + output_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 : Repeat N times loop Step3-Step8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Untrained Model Prediction: tensor([[0, 2, 7, 2, 2, 2, 2, 2, 2, 2]])\n"
     ]
    }
   ],
   "source": [
    "tgt = torch.zeros(1, 1).type_as(src)\n",
    "\n",
    "for i in range(9):\n",
    "    tgt_mask = following_mask(tgt.size(1)).type_as(src.data)\n",
    "\n",
    "    out = test_model.decode(memory, src_mask, tgt, tgt_mask)\n",
    "\n",
    "    prob = test_model.generator(out[:, -1])\n",
    "\n",
    "    next_word = torch.argmax(prob, dim=1).unsqueeze(0)\n",
    "\n",
    "    tgt=torch.cat([tgt, next_word],dim=1)\n",
    "\n",
    "print(\"Example Untrained Model Prediction:\", tgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Down-brake **Model.decode(memory, src_mask, tgt, tgt_mask)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Down-brake **Predict next word**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 2]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt = torch.zeros(1, 1).type_as(src)\n",
    "tgt_mask = following_mask(tgt.size(1)).type_as(src.data)\n",
    "\n",
    "prob = test_model.generator(out[:, -1])\n",
    "ext_word = torch.argmax(prob, dim=1).unsqueeze(0)\n",
    "\n",
    "tgt=torch.cat([tgt, next_word],dim=1)\n",
    "tgt\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMBa/2JyW7GewSx6ZS912U7",
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venvai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "0159b81555e194df7357f3ec66ac7a725116f7bdd06d05856166e90d27da3b92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
