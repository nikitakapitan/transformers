{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21815,
     "status": "ok",
     "timestamp": 1667234557531,
     "user": {
      "displayName": "Kapchenko Nikita",
      "userId": "13103833174454489193"
     },
     "user_tz": -60
    },
    "id": "SaOlNWG3sIxo",
    "outputId": "bbc85c13-4edc-4950-8b40-97b0ac0a49c8"
   },
   "outputs": [],
   "source": [
    "# Use below line for demo in external colabs\n",
    "#  !pip install -q git+https://github.com/nikitakapitan/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 381,
     "status": "ok",
     "timestamp": 1667235645442,
     "user": {
      "displayName": "Kapchenko Nikita",
      "userId": "13103833174454489193"
     },
     "user_tz": -60
    },
    "id": "WN8Jccx1sYXG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from transformers.main import make_model\n",
    "\n",
    "from transformers.Embeddings import Embeddings\n",
    "from transformers.PositionalEncoding import PositionalEncoding\n",
    "from transformers.MultiHeadedAttention import MultiHeadedAttention\n",
    "from transformers.helper import following_mask\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 1041,
     "status": "ok",
     "timestamp": 1667234558567,
     "user": {
      "displayName": "Kapchenko Nikita",
      "userId": "13103833174454489193"
     },
     "user_tz": -60
    },
    "id": "_hV9cbyH_59h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10]) torch.Size([1, 1, 10])\n"
     ]
    }
   ],
   "source": [
    "D_MODEL = 32\n",
    "D_FF = 2048\n",
    "DROPOUT = 0.1\n",
    "N = 2 #layers\n",
    "H = 8 #heads\n",
    "\n",
    "m = 10 #tokens in input\n",
    "\n",
    "\n",
    "FRENCH = 11 #SOURCE : number of all words in French  vocab\n",
    "ENGLISH  = 11 #TARGET : number of all words in English vocab\n",
    "\n",
    "# INITIALIZE\n",
    "test_model = make_model(src_vocab=FRENCH, tgt_vocab=ENGLISH, N=N)\n",
    "test_model.eval() # switch to eval (no DropOut & BatchNorm learning)\n",
    "\n",
    "# INFERENCE\n",
    "src = torch.LongTensor([range(1,m+1)])\n",
    "src_mask = torch.ones(1,1,m)\n",
    "print(src.shape, src_mask.shape)\n",
    "\n",
    "memory = test_model.encode(src, src_mask)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Untrained Model Prediction: tensor([[0, 9, 0, 9, 0, 5, 9, 9, 9, 9]])\n"
     ]
    }
   ],
   "source": [
    "ys = torch.zeros(1, 1).type_as(src)\n",
    "\n",
    "for i in range(9):\n",
    "    out = test_model.decode(\n",
    "        memory, src_mask, ys, following_mask(ys.size(1)).type_as(src.data)\n",
    "    )\n",
    "    prob = test_model.generator(out[:, -1])\n",
    "    _, next_word = torch.max(prob, dim=1)\n",
    "    next_word = next_word.data[0]\n",
    "    ys = torch.cat(\n",
    "        [ys, torch.empty(1, 1).type_as(src.data).fill_(next_word)], dim=1\n",
    "    )\n",
    "\n",
    "print(\"Example Untrained Model Prediction:\", ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CezVbVlt_bjl"
   },
   "source": [
    "# Down-brake **Model.encode(src, src_mask)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nmvZ88PbAO2O"
   },
   "source": [
    "## Embeddings(vocab, d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 381,
     "status": "ok",
     "timestamp": 1667237041900,
     "user": {
      "displayName": "Kapchenko Nikita",
      "userId": "13103833174454489193"
     },
     "user_tz": -60
    },
    "id": "oA9cbRs5_YyA",
    "outputId": "96f2ae2c-a05a-4588-916e-1f24b5487f83"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = src\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1667237042924,
     "user": {
      "displayName": "Kapchenko Nikita",
      "userId": "13103833174454489193"
     },
     "user_tz": -60
    },
    "id": "zsy6CDaDnVZI",
    "outputId": "f8869c79-8c3c-4773-9095-abe6b5237a78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 32])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = Embeddings(vocab=FRENCH, d_model=D_MODEL)\n",
    "x = emb(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rl8KmYjjAYpu"
   },
   "source": [
    "## PositionalEncoding(d_model, dropout, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "executionInfo": {
     "elapsed": 308,
     "status": "error",
     "timestamp": 1667237050666,
     "user": {
      "displayName": "Kapchenko Nikita",
      "userId": "13103833174454489193"
     },
     "user_tz": -60
    },
    "id": "Znjm0RX37t3h",
    "outputId": "cd699ef8-2d7f-41d9-b06d-2d6be65e777c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 32])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = 5000\n",
    "pos_enc = PositionalEncoding(d_model=D_MODEL, dropout=DROPOUT, max_len=MAX_LEN)\n",
    "x = pos_enc(x)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MultiHeadedAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 291,
     "status": "ok",
     "timestamp": 1667238034167,
     "user": {
      "displayName": "Kapchenko Nikita",
      "userId": "13103833174454489193"
     },
     "user_tz": -60
    },
    "id": "xevj1lyzCiy8",
    "outputId": "82e18b16-fa55-4513-9e6d-bd388c1076a7"
   },
   "outputs": [],
   "source": [
    "attn = MultiHeadedAttention(h=H, d_model=D_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "maylguAWDMwY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 1, 10])\n",
      "query shape= torch.Size([1, 10, 32])\n"
     ]
    }
   ],
   "source": [
    "# MultiHeadedAttention.__init__\n",
    "d_k = D_MODEL // H\n",
    "h = H\n",
    "q_fc = nn.Linear(D_MODEL, D_MODEL)\n",
    "k_fc = nn.Linear(D_MODEL, D_MODEL)\n",
    "v_fc = nn.Linear(D_MODEL, D_MODEL)\n",
    "final_fc = nn.Linear(D_MODEL, D_MODEL)\n",
    "dropout = nn.Dropout(p=DROPOUT)\n",
    "\n",
    "input = x\n",
    "mask = src_mask\n",
    "\n",
    "# MultiHeadedAttention.forward : compute Query, Key, Value\n",
    "mask = mask.unsqueeze(1); print(mask.shape)\n",
    "n_batches = input.size(0) # 1\n",
    "\n",
    "query = q_fc(input)\n",
    "key = k_fc(input)\n",
    "value = v_fc(input)\n",
    "print('query shape=', query.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "split data into H heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 10, 4])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split data into H heads.\n",
    "query = query.view(n_batches, m, h, d_k) .transpose(1, 2)\n",
    "key = key.view(n_batches, m, h, d_k).transpose(1, 2)\n",
    "value = value.view(n_batches, m, h, d_k).transpose(1, 2)\n",
    "query.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query.shape= torch.Size([1, 8, 10, 4])\n",
      "key_t.shape= torch.Size([1, 8, 4, 10])\n",
      "scores.shape= torch.Size([1, 8, 10, 10])\n",
      "p_attn.shape= torch.Size([1, 8, 10, 10])\n",
      "value.shape= torch.Size([1, 8, 10, 4])\n",
      "context.shape= torch.Size([1, 8, 10, 4])\n"
     ]
    }
   ],
   "source": [
    "# def attention\n",
    "key_t = key.transpose(-2, -1)\n",
    "print('query.shape=', query.shape)\n",
    "print('key_t.shape=', key_t.shape)\n",
    "\n",
    "scores = torch.matmul(query, key_t) / math.sqrt(d_k)\n",
    "print('scores.shape=', scores.shape)\n",
    "\n",
    "scores = scores.masked_fill(mask, -1e9)\n",
    "\n",
    "p_attn = scores.softmax(dim=-1)\n",
    "print('p_attn.shape=', p_attn.shape)\n",
    "\n",
    "# if dropout is not None:\n",
    "#     p_attn = dropout(p_attn)\n",
    "\n",
    "print('value.shape=', value.shape)\n",
    "context = torch.matmul(p_attn, value)\n",
    "print('context.shape=', context.shape)\n",
    "# return context, p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMBa/2JyW7GewSx6ZS912U7",
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venvai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "0159b81555e194df7357f3ec66ac7a725116f7bdd06d05856166e90d27da3b92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
