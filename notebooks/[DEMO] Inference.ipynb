{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21815,
     "status": "ok",
     "timestamp": 1667234557531,
     "user": {
      "displayName": "Kapchenko Nikita",
      "userId": "13103833174454489193"
     },
     "user_tz": -60
    },
    "id": "SaOlNWG3sIxo",
    "outputId": "bbc85c13-4edc-4950-8b40-97b0ac0a49c8"
   },
   "outputs": [],
   "source": [
    "# Use below line for demo in external colabs\n",
    "# !pip install -q torchdata==0.3.0 torchtext==0.12 spacy==3.2 altair GPUtil\n",
    "# !python -m spacy download de_core_news_sm\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !pip install -q git+https://github.com/nikitakapitan/transformers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 381,
     "status": "ok",
     "timestamp": 1667235645442,
     "user": {
      "displayName": "Kapchenko Nikita",
      "userId": "13103833174454489193"
     },
     "user_tz": -60
    },
    "id": "WN8Jccx1sYXG"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from copy import deepcopy\n",
    "from transformers.main import make_model\n",
    "\n",
    "from transformers.Embeddings import Embeddings\n",
    "from transformers.PositionalEncoding import PositionalEncoding\n",
    "from transformers.MultiHeadedAttention import MultiHeadedAttention\n",
    "from transformers.helper import following_mask\n",
    "from transformers.LayerNorm import LayerNorm\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 1041,
     "status": "ok",
     "timestamp": 1667234558567,
     "user": {
      "displayName": "Kapchenko Nikita",
      "userId": "13103833174454489193"
     },
     "user_tz": -60
    },
    "id": "_hV9cbyH_59h"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10]) torch.Size([1, 1, 10])\n"
     ]
    }
   ],
   "source": [
    "D_MODEL = 32 # should be 2^k and k>= n_heads\n",
    "D_FF = 2048\n",
    "N = 2 #layers\n",
    "H = 8 #heads\n",
    "\n",
    "n = 10 #tokens in input\n",
    "_dropout = 0.1\n",
    "MAX_LEN = 5000\n",
    "\n",
    "mapa = {1 : '1', 2 : '2', 3 : '3', 8 : 'h', 4 : 'd_head', \n",
    "        10 : 'n', 32 : 'emb', 2048 : 'd_ff' , 512: 'unkown'}\n",
    "\n",
    "\n",
    "FRENCH = 11 #SOURCE : number of all words in French  vocab\n",
    "ENGLISH  = 11 #TARGET : number of all words in English vocab\n",
    "\n",
    "# INITIALIZE\n",
    "test_model = make_model(src_vocab_len=FRENCH, tgt_vocab_len=ENGLISH, N=N, d_model=D_MODEL, d_ff = D_FF, h=H)\n",
    "test_model.eval() # switch to eval (no DropOut & BatchNorm learning)\n",
    "\n",
    "# INFERENCE\n",
    "src = torch.LongTensor([range(1,n+1)])\n",
    "src_mask = torch.ones(1,1,n)\n",
    "print(src.shape, src_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory.shape= ['1', 'n', 'emb']\n"
     ]
    }
   ],
   "source": [
    "memory = test_model.encode(src, src_mask)\n",
    "print('memory.shape=', [mapa[e] for e in memory.shape])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CezVbVlt_bjl"
   },
   "source": [
    "# Down-brake **Model.encode(src, src_mask)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 381,
     "status": "ok",
     "timestamp": 1667237041900,
     "user": {
      "displayName": "Kapchenko Nikita",
      "userId": "13103833174454489193"
     },
     "user_tz": -60
    },
    "id": "oA9cbRs5_YyA",
    "outputId": "96f2ae2c-a05a-4588-916e-1f24b5487f83"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 10]), <function Tensor.type>)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src.shape, src.type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1667237042924,
     "user": {
      "displayName": "Kapchenko Nikita",
      "userId": "13103833174454489193"
     },
     "user_tz": -60
    },
    "id": "zsy6CDaDnVZI",
    "outputId": "f8869c79-8c3c-4773-9095-abe6b5237a78"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 32])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_emb = Embeddings(vocab_len=FRENCH, d_model=D_MODEL)\n",
    "src = src_emb(src)\n",
    "src.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rl8KmYjjAYpu"
   },
   "source": [
    "## Step 2 PositionEncoding(d_model, max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 345
    },
    "executionInfo": {
     "elapsed": 308,
     "status": "error",
     "timestamp": 1667237050666,
     "user": {
      "displayName": "Kapchenko Nikita",
      "userId": "13103833174454489193"
     },
     "user_tz": -60
    },
    "id": "Znjm0RX37t3h",
    "outputId": "cd699ef8-2d7f-41d9-b06d-2d6be65e777c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 32])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_pos_enc = PositionalEncoding(d_model=D_MODEL, dropout=_dropout, max_len=MAX_LEN)\n",
    "src = src_pos_enc(src)\n",
    "src.shape #input_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >> Start Step 3 : ResidualConnection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_src = src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 MultiHeadedAttention\n",
    "\n",
    "attn = MultiHeadedAttention(h=H, d_model=D_MODEL)\n",
    "### Step 4.1 Query, Key, Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "maylguAWDMwY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask shape= torch.Size([1, 1, 1, 10])\n",
      "src.shape= ['1', 'n', 'emb']\n",
      "query.shape= ['1', 'n', 'emb']\n",
      "key.shape= ['1', 'n', 'emb']\n",
      "value.shape= ['1', 'n', 'emb']\n"
     ]
    }
   ],
   "source": [
    "# MultiHeadedAttention.__init__\n",
    "d_head = D_MODEL // H\n",
    "h = H\n",
    "q_fc = nn.Linear(D_MODEL, D_MODEL)\n",
    "k_fc = nn.Linear(D_MODEL, D_MODEL)\n",
    "v_fc = nn.Linear(D_MODEL, D_MODEL)\n",
    "final_fc = nn.Linear(D_MODEL, D_MODEL)\n",
    "dropout = nn.Dropout(p=_dropout)\n",
    "\n",
    "attn_from = src #input_3\n",
    "attn_to = src\n",
    "value = src\n",
    "mask = src_mask\n",
    "\n",
    "# MultiHeadedAttention.forward : compute Query, Key, Value\n",
    "mask = mask.unsqueeze(1)\n",
    "print('mask shape=',mask.shape)\n",
    "n_batches = src.size(0) # 1\n",
    "\n",
    "print('src.shape=', [mapa[e] for e in src.shape])\n",
    "query = q_fc(attn_from)\n",
    "key = k_fc(attn_to)\n",
    "value = v_fc(value)\n",
    "\n",
    "print('query.shape=', [mapa[e] for e in query.shape])\n",
    "print('key.shape=', [mapa[e] for e in key.shape])\n",
    "print('value.shape=', [mapa[e] for e in value.shape])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.2 Split to H heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 10, 4])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split data into H heads.\n",
    "query = query.view(n_batches, n, h, d_head) .transpose(1, 2)\n",
    "key = key.view(n_batches, n, h, d_head).transpose(1, 2)\n",
    "value = value.view(n_batches, n, h, d_head).transpose(1, 2)\n",
    "query.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4.3 Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query.shape= ['1', 'h', 'n', 'd_head']\n",
      "key_t.shape= ['1', 'h', 'd_head', 'n']\n",
      "scores.shape= ['1', 'h', 'n', 'n']\n",
      "p_attn.shape= ['1', 'h', 'n', 'n']\n",
      "value.shape= ['1', 'h', 'n', 'd_head']\n",
      "headed_context.shape= ['1', 'h', 'n', 'd_head']\n",
      "context.shape= ['1', 'n', 'emb']\n",
      "output_4.shape= ['1', 'n', 'emb']\n"
     ]
    }
   ],
   "source": [
    "# def attention\n",
    "key_t = key.transpose(-2, -1)\n",
    "print('query.shape=', [mapa[e] for e in query.shape])\n",
    "print('key_t.shape=', [mapa[e] for e in key_t.shape])\n",
    "\n",
    "scores = torch.matmul(query, key_t) / math.sqrt(d_head)\n",
    "print('scores.shape=', [mapa[e] for e in scores.shape])\n",
    "\n",
    "scores = scores.masked_fill(mask, -1e9)\n",
    "\n",
    "p_attn = scores.softmax(dim=-1)\n",
    "print('p_attn.shape=', [mapa[e] for e in p_attn.shape])\n",
    "\n",
    "# if dropout is not None:\n",
    "#     p_attn = dropout(p_attn)\n",
    "\n",
    "print('value.shape=', [mapa[e] for e in value.shape])\n",
    "headed_context = torch.matmul(p_attn, value)\n",
    "print('headed_context.shape=', [mapa[e] for e in headed_context.shape])\n",
    "\n",
    "context = headed_context.transpose(1,2).contiguous().view(n_batches, n, h * d_head)\n",
    "print('context.shape=', [mapa[e] for e in context.shape])\n",
    "\n",
    "src = final_fc(context) # output_4\n",
    "print('output_4.shape=', [mapa[e] for e in src.shape])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_5.shape= ['1', 'n', 'emb']\n"
     ]
    }
   ],
   "source": [
    "norm = LayerNorm(D_MODEL)\n",
    "src = norm(src) #output_5\n",
    "print('output_5.shape=', [mapa[e] for e in src.shape])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 Residual Connectnion (output 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_3.shape= ['1', 'n', 'emb']\n"
     ]
    }
   ],
   "source": [
    "# end ResidualConnection\n",
    "src = residual_src + src\n",
    "print('output_3.shape=', [mapa[e] for e in src.shape])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >> start Step 6 Residual Connecction (input 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_src = src"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7 PositionalWiseFeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc1.shape= ['1', 'n', 'd_ff']\n",
      "output_7.shape= ['1', 'n', 'emb']\n"
     ]
    }
   ],
   "source": [
    "w_1 = nn.Linear(D_MODEL, D_FF)\n",
    "w_2 = nn.Linear(D_FF, D_MODEL)\n",
    "\n",
    "fc1 = w_1(src).relu() # + DropOut\n",
    "print('fc1.shape=', [mapa[e] for e in fc1.shape])\n",
    "src = w_2(fc1) # output_7\n",
    "print('output_7.shape=', [mapa[e] for e in src.shape])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8 LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_8.shape= ['1', 'n', 'emb']\n"
     ]
    }
   ],
   "source": [
    "norm = LayerNorm(D_MODEL)\n",
    "src = norm(src) #output_8\n",
    "print('output_8.shape=', [mapa[e] for e in src.shape])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6 ResidualConnection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_6.shape= ['1', 'n', 'emb']\n"
     ]
    }
   ],
   "source": [
    "src = residual_src + src\n",
    "print('output_6.shape=', [mapa[e] for e in src.shape])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9 : Repeat N times loop Step3-Step8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tgt = torch.zeros(1, 1).type(torch.LongTensor)\n",
    "\n",
    "# for _ in range(9):\n",
    "#     tgt_mask = following_mask(tgt.size(1)).type_as(src.data)\n",
    "\n",
    "#     out = test_model.decode(memory, src_mask, tgt, tgt_mask)\n",
    "\n",
    "#     prob = test_model.generator(out[:, -1])\n",
    "\n",
    "#     next_word = torch.argmax(prob, dim=1).unsqueeze(0)\n",
    "\n",
    "#     tgt=torch.cat([tgt, next_word],dim=1)\n",
    "\n",
    "# print(\"Example Untrained Model Prediction:\", tgt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Down-brake **Model.decode(memory, src_mask, tgt, tgt_mask)**\n",
    "\n",
    "## Step 10 : initialize tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.zeros(1, 1).type(torch.LongTensor)\n",
    "\n",
    "tgt_mask = following_mask(target.size(1)).type_as(src.data)\n",
    "target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 11 : tgt_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 32])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_emb = Embeddings(vocab_len=ENGLISH, d_model=D_MODEL)\n",
    "tgt = tgt_emb(target)\n",
    "tgt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 12 : pos_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 32])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tgt_pos_enc = PositionalEncoding(d_model=D_MODEL, dropout=_dropout, max_len=MAX_LEN)\n",
    "tgt = tgt_pos_enc(tgt)\n",
    "tgt.shape # input_13\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >>Start Step 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "residual_tgt.shape= ['1', '1', 'emb']\n"
     ]
    }
   ],
   "source": [
    "residual_tgt = tgt\n",
    "print('residual_tgt.shape=', [mapa[e] for e in residual_tgt.shape])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 14 : Masked Multi-head Attention\n",
    "\n",
    "### Step 14.1 Query, Key, Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tgt_mask shape= torch.Size([1, 1, 1, 1])\n",
      "tgt.shape= ['1', '1', 'emb']\n",
      "query.shape= ['1', '1', 'emb']\n",
      "key.shape= ['1', '1', 'emb']\n",
      "value.shape= ['1', '1', 'emb']\n"
     ]
    }
   ],
   "source": [
    "# MultiHeadedAttention.__init__\n",
    "d_head = D_MODEL // H\n",
    "h = H\n",
    "q_fc = nn.Linear(D_MODEL, D_MODEL)\n",
    "k_fc = nn.Linear(D_MODEL, D_MODEL)\n",
    "v_fc = nn.Linear(D_MODEL, D_MODEL)\n",
    "final_fc = nn.Linear(D_MODEL, D_MODEL)\n",
    "dropout = nn.Dropout(p=_dropout)\n",
    "\n",
    "attn_from = tgt # input_13\n",
    "attn_to = tgt\n",
    "value = tgt\n",
    "mask = tgt_mask\n",
    "\n",
    "# MultiHeadedAttention.forward : compute Query, Key, Value\n",
    "mask = mask.unsqueeze(1)\n",
    "print('tgt_mask shape=',mask.shape)\n",
    "n_batches = tgt.size(0) # 1\n",
    "\n",
    "print('tgt.shape=', [mapa[e] for e in tgt.shape])\n",
    "query = q_fc(attn_from)\n",
    "key = k_fc(attn_to)\n",
    "value = v_fc(value)\n",
    "\n",
    "print('query.shape=', [mapa[e] for e in query.shape])\n",
    "print('key.shape=', [mapa[e] for e in key.shape])\n",
    "print('value.shape=', [mapa[e] for e in value.shape])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 14.2 Split to H heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 1, 4])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split data into H heads.\n",
    "n_tokens = query.size(1)\n",
    "query = query.view(n_batches, n_tokens, h, d_head) .transpose(1, 2)\n",
    "key = key.view(n_batches, n_tokens, h, d_head).transpose(1, 2)\n",
    "value = value.view(n_batches, n_tokens, h, d_head).transpose(1, 2)\n",
    "query.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 14.3 Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query.shape= ['1', 'h', '1', 'd_head']\n",
      "key_t.shape= ['1', 'h', 'd_head', '1']\n",
      "scores.shape= ['1', 'h', '1', '1']\n",
      "p_attn.shape= ['1', 'h', '1', '1']\n",
      "value.shape= ['1', 'h', '1', 'd_head']\n",
      "headed_context.shape= ['1', 'h', '1', 'd_head']\n",
      "context.shape= ['1', '1', 'emb']\n",
      "output_14.shape= ['1', '1', 'emb']\n"
     ]
    }
   ],
   "source": [
    "# def attention\n",
    "key_t = key.transpose(-2, -1)\n",
    "print('query.shape=', [mapa[e] for e in query.shape])\n",
    "print('key_t.shape=', [mapa[e] for e in key_t.shape])\n",
    "\n",
    "scores = torch.matmul(query, key_t) / math.sqrt(d_head)\n",
    "print('scores.shape=', [mapa[e] for e in scores.shape])\n",
    "\n",
    "scores = scores.masked_fill(mask, -1e9)\n",
    "\n",
    "p_attn = scores.softmax(dim=-1)\n",
    "print('p_attn.shape=', [mapa[e] for e in p_attn.shape])\n",
    "\n",
    "# if dropout is not None:\n",
    "#     p_attn = dropout(p_attn)\n",
    "\n",
    "print('value.shape=', [mapa[e] for e in value.shape])\n",
    "headed_context = torch.matmul(p_attn, value)\n",
    "print('headed_context.shape=', [mapa[e] for e in headed_context.shape])\n",
    "\n",
    "context = headed_context.transpose(1,2).contiguous().view(n_batches, n_tokens, h * d_head)\n",
    "print('context.shape=', [mapa[e] for e in context.shape])\n",
    "\n",
    "tgt = final_fc(context) # output_14\n",
    "print('output_14.shape=', [mapa[e] for e in tgt.shape])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 15 LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_15.shape= ['1', '1', 'emb']\n"
     ]
    }
   ],
   "source": [
    "norm = LayerNorm(D_MODEL)\n",
    "tgt = norm(tgt) #output_15\n",
    "print('output_15.shape=', [mapa[e] for e in tgt.shape])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 13 ResidualConnection (output 13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_13.shape= ['1', '1', 'emb']\n"
     ]
    }
   ],
   "source": [
    "# end ResidualConnection\n",
    "tgt = residual_tgt + tgt\n",
    "print('output_13.shape=', [mapa[e] for e in tgt.shape])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### start Step 16 : ResidualConnection (input 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_tgt= tgt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 17 : Multi-headed attention (2x)\n",
    "### Step 17.1 Query, Key, Value\n",
    " First appearance of 'memory' variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask shape= torch.Size([1, 1, 1, 10])\n",
      "memory.shape= ['1', 'n', 'emb']\n",
      "tgt.shape= ['1', '1', 'emb']\n",
      "query.shape= ['1', '1', 'emb']\n",
      "key.shape= ['1', 'n', 'emb']\n",
      "value.shape= ['1', 'n', 'emb']\n"
     ]
    }
   ],
   "source": [
    "# MultiHeadedAttention.__init__\n",
    "d_head = D_MODEL // H\n",
    "h = H\n",
    "q_fc = nn.Linear(D_MODEL, D_MODEL)\n",
    "k_fc = nn.Linear(D_MODEL, D_MODEL)\n",
    "v_fc = nn.Linear(D_MODEL, D_MODEL)\n",
    "final_fc = nn.Linear(D_MODEL, D_MODEL)\n",
    "dropout = nn.Dropout(p=_dropout)\n",
    "\n",
    "attn_from = tgt    #(b, dyn, emb)\n",
    "attn_to = memory   #(b, n, emb)\n",
    "value = memory     #(b, n, emb)\n",
    "mask = src_mask\n",
    "\n",
    "# MultiHeadedAttention.forward : compute Query, Key, Value\n",
    "mask = mask.unsqueeze(1)\n",
    "print('mask shape=',mask.shape)\n",
    "n_batches = src.size(0) # 1\n",
    "\n",
    "print('memory.shape=', [mapa[e] for e in memory.shape])\n",
    "print('tgt.shape=', [mapa[e] for e in tgt.shape])\n",
    "\n",
    "query = q_fc(attn_from) #(b, dyn, emb)\n",
    "key = k_fc(attn_to)     #(b, n, emb)\n",
    "value = v_fc(value)     #(b, n, emb)\n",
    "\n",
    "print('query.shape=', [mapa[e] for e in query.shape])\n",
    "print('key.shape=', [mapa[e] for e in key.shape])\n",
    "print('value.shape=', [mapa[e] for e in value.shape])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 17.2 Split to H heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query.shape= ['1', 'h', '1', 'd_head']\n",
      "key.shape= ['1', 'h', 'n', 'd_head']\n",
      "value.shape= ['1', 'h', 'n', 'd_head']\n"
     ]
    }
   ],
   "source": [
    "# split data into H heads.\n",
    "n_tokens_from = attn_from.size(1)\n",
    "n_tokens_to = attn_to.size(1)\n",
    "n_tokens_value = value.size(1)\n",
    "\n",
    "query = query.view(n_batches, n_tokens_from, h, d_head) .transpose(1, 2)\n",
    "key = key.view(n_batches, n_tokens_to, h, d_head).transpose(1, 2)\n",
    "value = value.view(n_batches, n_tokens_value, h, d_head).transpose(1, 2)\n",
    "\n",
    "print('query.shape=', [mapa[e] for e in query.shape])\n",
    "print('key.shape=', [mapa[e] for e in key.shape])\n",
    "print('value.shape=', [mapa[e] for e in value.shape])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 17.3 Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "query.shape= ['1', 'h', '1', 'd_head']\n",
      "key_t.shape= ['1', 'h', 'd_head', 'n']\n",
      "scores.shape= ['1', 'h', '1', 'n']\n",
      "p_attn.shape= ['1', 'h', '1', 'n']\n",
      "value.shape= ['1', 'h', 'n', 'd_head']\n",
      "headed_context.shape= ['1', 'h', '1', 'd_head']\n",
      "context.shape= ['1', '1', 'emb']\n",
      "tgt.shape= ['1', '1', 'emb']\n"
     ]
    }
   ],
   "source": [
    "# def attention\n",
    "key_t = key.transpose(-2, -1)\n",
    "print('query.shape=', [mapa[e] for e in query.shape])\n",
    "print('key_t.shape=', [mapa[e] for e in key_t.shape])\n",
    "\n",
    "scores = torch.matmul(query, key_t) / math.sqrt(d_head)\n",
    "print('scores.shape=', [mapa[e] for e in scores.shape])\n",
    "\n",
    "scores = scores.masked_fill(mask, -1e9)\n",
    "\n",
    "p_attn = scores.softmax(dim=-1)\n",
    "print('p_attn.shape=', [mapa[e] for e in p_attn.shape])\n",
    "\n",
    "# if dropout is not None:\n",
    "#     p_attn = dropout(p_attn)\n",
    "\n",
    "print('value.shape=', [mapa[e] for e in value.shape])\n",
    "headed_context = torch.matmul(p_attn, value)\n",
    "print('headed_context.shape=', [mapa[e] for e in headed_context.shape])\n",
    "\n",
    "context = headed_context.transpose(1,2).contiguous().view(n_batches, n_tokens_from, h * d_head)\n",
    "print('context.shape=', [mapa[e] for e in context.shape])\n",
    "\n",
    "tgt = final_fc(context) # output_4\n",
    "print('tgt.shape=', [mapa[e] for e in tgt.shape])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 18 LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_18.shape= ['1', '1', 'emb']\n"
     ]
    }
   ],
   "source": [
    "norm = LayerNorm(D_MODEL)\n",
    "tgt = norm(tgt) #output_18\n",
    "print('output_18.shape=', [mapa[e] for e in tgt.shape])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 16 : ResidualConnection (output 16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_16.shape= ['1', '1', 'emb']\n"
     ]
    }
   ],
   "source": [
    "tgt = residual_tgt + tgt\n",
    "print('output_16.shape=', [mapa[e] for e in tgt.shape])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### >> Start Step 19 : ResidualCOnnection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_tgt = tgt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 20 PositionalWiseFeedForward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tgt_fc1.shape= ['1', '1', 'd_ff']\n",
      "output_20.shape= ['1', '1', 'emb']\n"
     ]
    }
   ],
   "source": [
    "w_1 = nn.Linear(D_MODEL, D_FF)\n",
    "w_2 = nn.Linear(D_FF, D_MODEL)\n",
    "\n",
    "tgt_fc1 = w_1(tgt).relu() # + DropOut\n",
    "print('tgt_fc1.shape=', [mapa[e] for e in tgt_fc1.shape])\n",
    "tgt = w_2(tgt_fc1) # output_20\n",
    "print('output_20.shape=', [mapa[e] for e in tgt.shape])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 21 LayerNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_21.shape= ['1', '1', 'emb']\n"
     ]
    }
   ],
   "source": [
    "norm = LayerNorm(D_MODEL)\n",
    "tgt = norm(tgt) #output_8\n",
    "print('output_21.shape=', [mapa[e] for e in tgt.shape])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 19 ResidualConnection (output 19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_19.shape= ['1', '1', 'emb']\n"
     ]
    }
   ],
   "source": [
    "tgt = residual_tgt+ tgt\n",
    "print('output_19.shape=', [mapa[e] for e in tgt.shape])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 22 : Generator & Update tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tgt.shape= ['1', '1', 'emb']\n",
      "prev target.shape= ['1', '1']\n",
      "next_word.shape= ['1', '1']\n",
      "curent target.shape= ['1', '2']\n"
     ]
    }
   ],
   "source": [
    "prob = test_model.generator(tgt[:, -1])\n",
    "next_word = torch.argmax(prob, dim=1).unsqueeze(0)\n",
    "print('tgt.shape=', [mapa[e] for e in tgt.shape])\n",
    "print('prev target.shape=', [mapa[e] for e in target.shape])\n",
    "print('next_word.shape=', [mapa[e] for e in next_word.shape])\n",
    "\n",
    "target=torch.cat([target, next_word],dim=1)\n",
    "print('curent target.shape=', [mapa[e] for e in target.shape])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 23 : Repeat N times loop step 13 -> step 22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current target.shape= ['1', '3']\n"
     ]
    }
   ],
   "source": [
    "tgt_mask = following_mask(target.size(1)).type_as(src.data)\n",
    "\n",
    "tgt = tgt_emb(target)\n",
    "tgt = tgt_pos_enc(tgt)\n",
    "\n",
    "residual_tgt = tgt\n",
    "\n",
    "attn_from = tgt \n",
    "attn_to = tgt\n",
    "value = tgt\n",
    "mask = tgt_mask.unsqueeze(1)\n",
    "n_batches = tgt.size(0) \n",
    "\n",
    "query = q_fc(attn_from)\n",
    "key = k_fc(attn_to)\n",
    "value = v_fc(value)\n",
    "n_tokens = query.size(1)\n",
    "\n",
    "query = query.view(n_batches, n_tokens, h, d_head) .transpose(1, 2)\n",
    "key = key.view(n_batches, n_tokens, h, d_head).transpose(1, 2)\n",
    "value = value.view(n_batches, n_tokens, h, d_head).transpose(1, 2)\n",
    "\n",
    "key_t = key.transpose(-2, -1)\n",
    "scores = torch.matmul(query, key_t) / math.sqrt(d_head)\n",
    "scores = scores.masked_fill(mask, -1e9)\n",
    "p_attn = scores.softmax(dim=-1)\n",
    "headed_context = torch.matmul(p_attn, value)\n",
    "context = headed_context.transpose(1,2).contiguous().view(n_batches, n_tokens, h * d_head)\n",
    "tgt = final_fc(context) \n",
    "\n",
    "tgt = norm(tgt) \n",
    "tgt = residual_tgt + tgt\n",
    "\n",
    "residual_tgt= tgt\n",
    "\n",
    "attn_from = tgt\n",
    "attn_to = memory\n",
    "value = memory\n",
    "\n",
    "mask = src_mask.unsqueeze(1)\n",
    "n_batches = src.size(0) \n",
    "\n",
    "query = q_fc(attn_from)\n",
    "key = k_fc(attn_to)\n",
    "value = v_fc(value)\n",
    "\n",
    "n_tokens_from = attn_from.size(1)\n",
    "n_tokens_to = attn_to.size(1)\n",
    "n_tokens_value = value.size(1)\n",
    "\n",
    "query = query.view(n_batches, n_tokens_from, h, d_head) .transpose(1, 2)\n",
    "key = key.view(n_batches, n_tokens_to, h, d_head).transpose(1, 2)\n",
    "value = value.view(n_batches, n_tokens_value, h, d_head).transpose(1, 2)\n",
    "\n",
    "key_t = key.transpose(-2, -1)\n",
    "scores = torch.matmul(query, key_t) / math.sqrt(d_head)\n",
    "scores = scores.masked_fill(mask, -1e9)\n",
    "p_attn = scores.softmax(dim=-1)\n",
    "headed_context = torch.matmul(p_attn, value)\n",
    "context = headed_context.transpose(1,2).contiguous().view(n_batches, n_tokens_from, h * d_head)\n",
    "tgt = final_fc(context) \n",
    "\n",
    "tgt = norm(tgt)\n",
    "tgt = residual_tgt + tgt\n",
    "\n",
    "residual_tgt = tgt\n",
    "\n",
    "tgt_fc1 = w_1(tgt).relu() \n",
    "tgt = w_2(tgt_fc1) \n",
    "\n",
    "tgt = norm(tgt)\n",
    "tgt = residual_tgt+ tgt\n",
    "\n",
    "prob = test_model.generator(tgt[:, -1])\n",
    "next_word = torch.argmax(prob, dim=1).unsqueeze(0)\n",
    "\n",
    "target=torch.cat([target, next_word],dim=1)\n",
    "\n",
    "print('current target.shape=', [mapa[e] for e in target.shape])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMBa/2JyW7GewSx6ZS912U7",
   "collapsed_sections": [],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venvai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "0159b81555e194df7357f3ec66ac7a725116f7bdd06d05856166e90d27da3b92"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
